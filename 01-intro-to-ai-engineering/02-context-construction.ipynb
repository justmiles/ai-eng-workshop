{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Construction Best Practices\n",
    "\n",
    "This notebook demonstrates four key principles for effective prompt engineering:\n",
    "\n",
    "1. **Position matters**: Models process information at the beginning and end of prompts better than content in the middle\n",
    "2. **Be mindful of context limits**: Track token usage and prioritize the most relevant information\n",
    "3. **Dynamic context**: Tailor context to each query rather than using static templates\n",
    "4. **Format appropriately**: Present data as concise summaries rather than raw dumps\n",
    "\n",
    "Let's explore each principle with practical examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "!pip install litellm tiktoken\n",
    "from litellm import completion\n",
    "import tiktoken\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model = \"ollama/driaforall/tiny-agent-a:0.5b\"\n",
    "\n",
    "# Helper function to count tokens (approximate)\n",
    "def count_tokens(text):\n",
    "    \"\"\"Approximate token count\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(encoding.encode(str(text)))\n",
    "    except:\n",
    "        # Fallback approximation\n",
    "        return len(str(text)) // 4\n",
    "\n",
    "def get_response(messages, show_tokens=True):\n",
    "    \"\"\"Helper function to get response and show token usage\"\"\"\n",
    "    if show_tokens:\n",
    "        total_tokens = sum(count_tokens(msg[\"content\"]) for msg in messages)\n",
    "        print(f\"Estimated tokens used: {total_tokens}\")\n",
    "    \n",
    "    response = completion(model=model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Position Matters: Beginning and End Are Key\n",
    "\n",
    "Models exhibit **recency bias** and **primacy bias** - they pay more attention to information at the beginning and end of the context than content buried in the middle.\n",
    "\n",
    "###  Bad Example: Important Information Buried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad: Critical information hidden in the middle\n",
    "bad_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    I need help with my project. Here's some background: I work at a tech company.\n",
    "    We use various tools and technologies. Our team is diverse and talented.\n",
    "    The project involves machine learning and data processing.\n",
    "    CRITICAL: The deadline is tomorrow and I need to focus only on the deployment checklist.\n",
    "    We've been working on this for months. The stakeholders are eager to see results.\n",
    "    What should I do to prepare for the presentation?\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "print(\" Bad Example Response:\")\n",
    "bad_response = get_response(bad_messages)\n",
    "print(bad_response)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Good Example: Critical Information at Beginning and End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Critical information prominently positioned\n",
    "good_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "    You are a deployment specialist. PRIORITY: Focus on actionable deployment checklists.\n",
    "    \"\"\"}, \n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    URGENT: Deployment deadline is tomorrow. Need deployment checklist focus only.\n",
    "    \n",
    "    Context: ML project at tech company, months of development, stakeholder presentation.\n",
    "    \n",
    "    QUESTION: What deployment checklist items should I prioritize for tomorrow's deadline?\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "print(\"Good Example Response:\")\n",
    "good_response = get_response(good_messages)\n",
    "print(good_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Context Limits: Token Management\n",
    "\n",
    "Most models have context windows (e.g., 4K, 8K, 32K tokens). Exceeding these limits causes truncation or errors.\n",
    "\n",
    "### Demonstration: Tracking Token Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Building context while monitoring tokens\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "print(\"Token usage as we build context:\")\n",
    "print(f\"System prompt: {count_tokens(messages[0]['content'])} tokens\")\n",
    "\n",
    "# Add user message\n",
    "user_msg = \"What are the best practices for API design?\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "print(f\"+ User message: {count_tokens(user_msg)} tokens\")\n",
    "print(f\"Total so far: {sum(count_tokens(msg['content']) for msg in messages)} tokens\")\n",
    "\n",
    "# Simulate adding large context (like documentation)\n",
    "large_context = \"Here's our entire API documentation: \" + \"API endpoint details. \" * 200\n",
    "print(f\"\\nLarge context would add: {count_tokens(large_context)} tokens\")\n",
    "print(\" This might exceed context limits!\")\n",
    "\n",
    "# Better approach: Summarized context\n",
    "summarized_context = \"\"\"\n",
    "Key API principles from docs:\n",
    "- RESTful design patterns\n",
    "- Consistent error handling\n",
    "- Rate limiting implementation\n",
    "- Authentication best practices\n",
    "\"\"\"\n",
    "print(f\"\\nSummarized context: {count_tokens(summarized_context)} tokens\")\n",
    "print(\" Much more efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Managing Conversation History in Context\n",
    "\n",
    "Including previous messages maintains conversation continuity, but requires careful management to avoid exceeding context limits.\n",
    "\n",
    "###  Bad Example: Including All History Without Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad: Blindly including all conversation history\n",
    "def bad_conversation_management():\n",
    "    # Simulated long conversation history\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the difference between lists and tuples in Python?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Lists are mutable (can be changed) while tuples are immutable (cannot be changed). Lists use square brackets [] and tuples use parentheses (). Lists are better for data that might change, tuples for fixed data.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you show me some examples of when to use each?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure! Use lists for: shopping lists, user data that updates, dynamic collections. Use tuples for: coordinates (x,y), database records, configuration settings, function return values when returning multiple items.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What about performance differences?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Tuples are generally faster for accessing elements and take less memory. Lists are slower due to their mutability features but more flexible. For large datasets where you don't need to modify data, tuples can be significantly more efficient.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How do I convert between them?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Use list() to convert tuple to list: list((1,2,3)) gives [1,2,3]. Use tuple() to convert list to tuple: tuple([1,2,3]) gives (1,2,3). Both create new objects, so original is unchanged.\"},\n",
    "        # ... many more messages\n",
    "    ]\n",
    "    \n",
    "    # New question - including ALL history\n",
    "    new_question = \"Now I want to learn about dictionaries in Python.\"\n",
    "    \n",
    "    # This creates a very long context\n",
    "    full_messages = conversation_history + [{\"role\": \"user\", \"content\": new_question}]\n",
    "    \n",
    "    total_tokens = sum(count_tokens(msg['content']) for msg in full_messages)\n",
    "    print(f\" Bad approach - Total tokens: {total_tokens}\")\n",
    "    print(f\"Number of messages: {len(full_messages)}\")\n",
    "    print(\" This will quickly exceed context limits!\")\n",
    "    \n",
    "    return full_messages\n",
    "\n",
    "bad_messages = bad_conversation_management()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Good Example: Smart Conversation History Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Intelligent conversation history management\n",
    "def smart_conversation_management(conversation_history, new_question, max_history_tokens=500):\n",
    "    \"\"\"\n",
    "    Manage conversation history intelligently:\n",
    "    1. Keep system prompt\n",
    "    2. Summarize or truncate old messages\n",
    "    3. Keep recent context\n",
    "    4. Stay within token limits\n",
    "    \"\"\"\n",
    "    \n",
    "    # Always keep the system prompt\n",
    "    system_prompt = conversation_history[0]\n",
    "    recent_messages = conversation_history[1:]  # All messages except system\n",
    "    \n",
    "    # Strategy 1: Keep only recent messages that fit in token limit\n",
    "    selected_messages = [system_prompt]\n",
    "    current_tokens = count_tokens(system_prompt['content'])\n",
    "    \n",
    "    # Add messages from most recent backwards until we hit limit\n",
    "    for message in reversed(recent_messages):\n",
    "        msg_tokens = count_tokens(message['content'])\n",
    "        if current_tokens + msg_tokens <= max_history_tokens:\n",
    "            selected_messages.insert(1, message)  # Insert after system prompt\n",
    "            current_tokens += msg_tokens\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Add new question\n",
    "    selected_messages.append({\"role\": \"user\", \"content\": new_question})\n",
    "    \n",
    "    return selected_messages, current_tokens\n",
    "\n",
    "# Test with the same conversation\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the difference between lists and tuples in Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Lists are mutable, tuples are immutable. Lists use [], tuples use ().\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you show me examples?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Lists: shopping_list = ['milk', 'eggs']. Tuples: coordinates = (10, 20).\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about performance differences?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Tuples are faster and use less memory due to immutability.\"},\n",
    "]\n",
    "\n",
    "new_question = \"Now I want to learn about dictionaries in Python.\"\n",
    "\n",
    "smart_messages, tokens_used = smart_conversation_management(conversation_history, new_question)\n",
    "\n",
    "print(\" Smart conversation management:\")\n",
    "print(f\"Selected messages: {len(smart_messages)}\")\n",
    "print(f\"Total tokens: {tokens_used + count_tokens(new_question)}\")\n",
    "print(\"\\nMessage structure:\")\n",
    "for i, msg in enumerate(smart_messages):\n",
    "    role = msg['role']\n",
    "    content_preview = msg['content'][:50] + \"...\" if len(msg['content']) > 50 else msg['content']\n",
    "    print(f\"{i+1}. {role.title()}: {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Strategy: Conversation Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Summarize older conversation parts\n",
    "def advanced_conversation_management(conversation_history, new_question, max_tokens=800):\n",
    "    \"\"\"\n",
    "    Advanced strategy:\n",
    "    1. Keep system prompt\n",
    "    2. Summarize older conversation parts\n",
    "    3. Keep recent detailed exchanges\n",
    "    4. Add new question\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = conversation_history[0]\n",
    "    messages = conversation_history[1:]\n",
    "    \n",
    "    # Keep last 4 messages (2 exchanges) for full context\n",
    "    recent_messages = messages[-4:] if len(messages) >= 4 else messages\n",
    "    older_messages = messages[:-4] if len(messages) >= 4 else []\n",
    "    \n",
    "    # Create summary of older conversation\n",
    "    if older_messages:\n",
    "        # Extract key topics from older messages\n",
    "        topics = []\n",
    "        for msg in older_messages:\n",
    "            if msg['role'] == 'user':\n",
    "                # Extract main topic from user questions\n",
    "                content = msg['content'].lower()\n",
    "                if 'list' in content and 'tuple' in content:\n",
    "                    topics.append(\"lists vs tuples\")\n",
    "                elif 'example' in content:\n",
    "                    topics.append(\"practical examples\")\n",
    "        \n",
    "        summary = f\"Previous discussion covered: {', '.join(set(topics))}\"\n",
    "        summary_message = {\"role\": \"system\", \"content\": f\"Context: {summary}\"}\n",
    "    else:\n",
    "        summary_message = None\n",
    "    \n",
    "    # Build final message list\n",
    "    final_messages = [system_prompt]\n",
    "    \n",
    "    if summary_message:\n",
    "        final_messages.append(summary_message)\n",
    "    \n",
    "    final_messages.extend(recent_messages)\n",
    "    final_messages.append({\"role\": \"user\", \"content\": new_question})\n",
    "    \n",
    "    total_tokens = sum(count_tokens(msg['content']) for msg in final_messages)\n",
    "    \n",
    "    return final_messages, total_tokens\n",
    "\n",
    "# Test advanced strategy\n",
    "advanced_messages, advanced_tokens = advanced_conversation_management(conversation_history, new_question)\n",
    "\n",
    "print(\" Advanced conversation management:\")\n",
    "print(f\"Total messages: {len(advanced_messages)}\")\n",
    "print(f\"Total tokens: {advanced_tokens}\")\n",
    "print(\"\\nStructure:\")\n",
    "for i, msg in enumerate(advanced_messages):\n",
    "    role = msg['role']\n",
    "    content_preview = msg['content'][:60] + \"...\" if len(msg['content']) > 60 else msg['content']\n",
    "    tokens = count_tokens(msg['content'])\n",
    "    print(f\"{i+1}. {role.title()} ({tokens} tokens): {content_preview}\")\n",
    "\n",
    "# Get response using advanced strategy\n",
    "print(\"\\n Response with proper conversation context:\")\n",
    "advanced_response = get_response(advanced_messages, show_tokens=False)\n",
    "print(advanced_response[:200] + \"...\" if len(advanced_response) > 200 else advanced_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Strategies for Conversation History:\n",
    "\n",
    "1. **Always preserve the system prompt** - It defines the assistant's behavior\n",
    "\n",
    "2. **Prioritize recent messages** - They provide immediate context\n",
    "\n",
    "3. **Summarize older exchanges** - Capture key topics without full detail\n",
    "\n",
    "4. **Monitor token usage** - Set limits and stick to them\n",
    "\n",
    "5. **Consider message relevance** - Some older messages may be more important than recent ones\n",
    "\n",
    "### Token Comparison:\n",
    "-  **All history**: Can easily exceed 2000+ tokens\n",
    "-  **Smart selection**: ~300-500 tokens\n",
    "-  **Advanced summarization**: ~400-600 tokens with better context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic vs Static Context\n",
    "\n",
    "Tailor your context to each specific query rather than using one-size-fits-all templates.\n",
    "\n",
    "###  Static Context Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static context - same for all queries\n",
    "static_system_prompt = \"\"\"\n",
    "You are a coding assistant. You help with Python, JavaScript, API design, \n",
    "database queries, debugging, testing, deployment, security, performance, \n",
    "documentation, code review, and general software development questions.\n",
    "\"\"\"\n",
    "\n",
    "# Query 1: Simple Python question\n",
    "query1_messages = [\n",
    "    {\"role\": \"system\", \"content\": static_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"How do I reverse a string in Python?\"}\n",
    "]\n",
    "\n",
    "print(\" Static context for simple Python question:\")\n",
    "print(f\"System prompt tokens: {count_tokens(static_system_prompt)}\")\n",
    "response1 = get_response(query1_messages, show_tokens=False)\n",
    "print(f\"Response: {response1[:100]}...\")\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dynamic Context Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic context - tailored to specific query\n",
    "def create_dynamic_context(query_type, specific_context=\"\"):\n",
    "    base_contexts = {\n",
    "        \"python_basics\": \"You are a Python tutor. Focus on clear, beginner-friendly explanations with examples.\",\n",
    "        \"api_design\": \"You are an API architect. Focus on RESTful principles, best practices, and scalability.\",\n",
    "        \"debugging\": \"You are a debugging expert. Focus on systematic troubleshooting approaches.\",\n",
    "        \"security\": \"You are a security specialist. Focus on secure coding practices and threat mitigation.\"\n",
    "    }\n",
    "    \n",
    "    context = base_contexts.get(query_type, \"You are a helpful assistant.\")\n",
    "    if specific_context:\n",
    "        context += f\" Additional context: {specific_context}\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Same query with dynamic context\n",
    "dynamic_system_prompt = create_dynamic_context(\"python_basics\")\n",
    "\n",
    "query1_dynamic = [\n",
    "    {\"role\": \"system\", \"content\": dynamic_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"How do I reverse a string in Python?\"}\n",
    "]\n",
    "\n",
    "print(\" Dynamic context for simple Python question:\")\n",
    "print(f\"System prompt tokens: {count_tokens(dynamic_system_prompt)}\")\n",
    "response1_dynamic = get_response(query1_dynamic, show_tokens=False)\n",
    "print(f\"Response: {response1_dynamic[:100]}...\")\n",
    "\n",
    "print(f\"\\n Token savings: {count_tokens(static_system_prompt) - count_tokens(dynamic_system_prompt)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Proper Formatting: Summaries vs Raw Dumps\n",
    "\n",
    "Present information in digestible, structured formats rather than overwhelming raw data.\n",
    "\n",
    "###  Bad Example: Raw Data Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated raw database results\n",
    "raw_data = \"\"\"\n",
    "Database results:\n",
    "id: 1, name: John Smith, email: john@example.com, department: Engineering, salary: 85000, hire_date: 2022-01-15, manager_id: 5, project_count: 3, last_review: 2023-08-10, performance_rating: 4.2\n",
    "id: 2, name: Sarah Johnson, email: sarah@example.com, department: Engineering, salary: 92000, hire_date: 2021-03-22, manager_id: 5, project_count: 5, last_review: 2023-07-25, performance_rating: 4.7\n",
    "id: 3, name: Mike Chen, email: mike@example.com, department: Marketing, salary: 78000, hire_date: 2023-02-01, manager_id: 8, project_count: 2, last_review: 2023-09-15, performance_rating: 3.9\n",
    "\"\"\"\n",
    "\n",
    "bad_context_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an HR assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{raw_data}\\n\\nWho are our top performers in Engineering?\"}\n",
    "]\n",
    "\n",
    "print(\" Bad: Raw data dump\")\n",
    "print(f\"Context size: {count_tokens(raw_data)} tokens\")\n",
    "bad_formatting_response = get_response(bad_context_messages, show_tokens=False)\n",
    "print(f\"Response: {bad_formatting_response}\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well-formatted summary\n",
    "structured_summary = \"\"\"\n",
    "Engineering Team Summary:\n",
    "- John Smith: 4.2 rating, 3 projects, hired Jan 2022\n",
    "- Sarah Johnson: 4.7 rating, 5 projects, hired Mar 2021\n",
    "\n",
    "Performance Metrics:\n",
    "- Average rating: 4.45\n",
    "- Total projects: 8\n",
    "- Team size: 2\n",
    "\"\"\"\n",
    "\n",
    "good_context_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an HR assistant specializing in performance analysis.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{structured_summary}\\n\\nWho are our top performers in Engineering?\"}\n",
    "]\n",
    "\n",
    "print(\" Good: Structured summary\")\n",
    "print(f\"Context size: {count_tokens(structured_summary)} tokens\")\n",
    "good_formatting_response = get_response(good_context_messages, show_tokens=False)\n",
    "print(f\"Response: {good_formatting_response}\")\n",
    "\n",
    "print(f\"\\n Token savings: {count_tokens(raw_data) - count_tokens(structured_summary)} tokens\")\n",
    "print(f\" Efficiency gain: {((count_tokens(raw_data) - count_tokens(structured_summary)) / count_tokens(raw_data) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Context Components in Action\n",
    "\n",
    "Let's see how different context components work together effectively.\n",
    "\n",
    "### Complete Context Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a complete context with all components\n",
    "def build_complete_context(user_query, conversation_history=None, retrieved_info=None, available_tools=None):\n",
    "    \n",
    "    # 1. System prompt (defines role and behavior)\n",
    "    system_prompt = \"\"\"\n",
    "    You are a senior software architect. You provide technical guidance based on best practices,\n",
    "    consider scalability, maintainability, and security in your recommendations.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt.strip()}]\n",
    "    \n",
    "    # 2. Add conversation history (if any)\n",
    "    if conversation_history:\n",
    "        messages.extend(conversation_history)\n",
    "    \n",
    "    # 3. Build user message with retrieved info and tools\n",
    "    user_content = \"\"\n",
    "    \n",
    "    # Add retrieved information (RAG)\n",
    "    if retrieved_info:\n",
    "        user_content += f\"Relevant context:\\n{retrieved_info}\\n\\n\"\n",
    "    \n",
    "    # Add available tools\n",
    "    if available_tools:\n",
    "        user_content += f\"Available tools: {', '.join(available_tools)}\\n\\n\"\n",
    "    \n",
    "    # Add the actual user query\n",
    "    user_content += f\"Question: {user_query}\"\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Example usage\n",
    "conversation_history = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm building a social media app.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Great! What's your main technical challenge?\"}\n",
    "]\n",
    "\n",
    "retrieved_info = \"\"\"\n",
    "Database performance tips:\n",
    "- Use connection pooling\n",
    "- Implement read replicas\n",
    "- Add proper indexing\n",
    "\"\"\"\n",
    "\n",
    "available_tools = [\"database_optimizer\", \"load_tester\", \"security_scanner\"]\n",
    "\n",
    "complete_messages = build_complete_context(\n",
    "    user_query=\"How should I optimize my database for handling user feeds?\",\n",
    "    conversation_history=conversation_history,\n",
    "    retrieved_info=retrieved_info,\n",
    "    available_tools=available_tools\n",
    ")\n",
    "\n",
    "print(\" Complete Context Structure:\")\n",
    "for i, msg in enumerate(complete_messages):\n",
    "    print(f\"{i+1}. {msg['role'].title()}: {count_tokens(msg['content'])} tokens\")\n",
    "    if msg['role'] == 'user':\n",
    "        print(f\"   Preview: {msg['content'][:100]}...\")\n",
    "\n",
    "print(\"\\n Response:\")\n",
    "complete_response = get_response(complete_messages)\n",
    "print(complete_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete these exercises to reinforce your learning.\n",
    "\n",
    "### Exercise 1: Fix the Position Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Rewrite this poorly positioned context\n",
    "poorly_positioned = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    I'm working on various projects. Some involve web development, others mobile apps.\n",
    "    My team uses different technologies and frameworks. We have deadlines to meet.\n",
    "    URGENT: I need to fix a critical security vulnerability in our authentication system NOW.\n",
    "    The system handles user login and session management. We use JWT tokens.\n",
    "    What general development tips do you have?\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "# TODO: Rewrite to properly position the critical information\n",
    "# Your solution here:\n",
    "fixed_positioning = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a security specialist. PRIORITY: Address critical vulnerabilities immediately.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    CRITICAL SECURITY ISSUE: Authentication vulnerability needs immediate fix.\n",
    "    \n",
    "    System details: JWT-based auth, handles login/sessions\n",
    "    Context: Web/mobile projects, team environment\n",
    "    \n",
    "    URGENT QUESTION: How do I fix this authentication vulnerability?\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "print(\"Exercise 1 - Your Solution:\")\n",
    "exercise1_response = get_response(fixed_positioning)\n",
    "print(exercise1_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create Dynamic Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create a dynamic context system\n",
    "def create_context_for_query(query, domain):\n",
    "    \"\"\"\n",
    "    Create appropriate context based on the query and domain.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        domain: The domain (e.g., 'security', 'performance', 'ui_ux', 'testing')\n",
    "    \n",
    "    Returns:\n",
    "        List of messages with appropriate context\n",
    "    \"\"\"\n",
    "    # TODO: Implement dynamic context creation\n",
    "    context_templates = {\n",
    "        'security': 'You are a cybersecurity expert. Focus on threat analysis and secure coding practices.',\n",
    "        'performance': 'You are a performance optimization specialist. Focus on efficiency and scalability.',\n",
    "        'ui_ux': 'You are a UX designer. Focus on user experience and interface design principles.',\n",
    "        'testing': 'You are a QA engineer. Focus on testing strategies and quality assurance.'\n",
    "    }\n",
    "    \n",
    "    system_content = context_templates.get(domain, 'You are a helpful assistant.')\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "# Test your function\n",
    "test_queries = [\n",
    "    (\"How do I prevent SQL injection?\", \"security\"),\n",
    "    (\"My app is loading slowly, what should I check?\", \"performance\"),\n",
    "    (\"How can I make my form more user-friendly?\", \"ui_ux\")\n",
    "]\n",
    "\n",
    "print(\"Exercise 2 - Testing Dynamic Context:\")\n",
    "for query, domain in test_queries:\n",
    "    messages = create_context_for_query(query, domain)\n",
    "    print(f\"\\nDomain: {domain}\")\n",
    "    print(f\"Tokens: {sum(count_tokens(msg['content']) for msg in messages)}\")\n",
    "    response = get_response(messages, show_tokens=False)\n",
    "    print(f\"Response preview: {response[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
